{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Implementing a chatbot using Llama-2-13b-chat, vLLM, LangChain, and the Gradio UI\n",
    "- [**Langchain**](https://python.langchain.com/docs/get_started/introduction) is a framework for the fast prototyping of applications powered by language models. In this example we replaced the Gradio's chatbot memory with LangChain's memory implemented by the `LLMChain` class.\n",
    "- [**vLLM**](https://vllm.readthedocs.io/en/latest/) is a fast and easy-to-use library for LLM inference and serving. In this example, we use LangChain's `VLLMOpenAI` class to access the vLLM OpenAI API-compatible service.\n",
    "- [**Gradio**](https://www.gradio.app/) is a fast way to demo machine learning models with a friendly web interface.\n",
    "\n",
    "### Requirements\n",
    "Please follow the instructions provided by the README.md file from the \n",
    "[LLM-fine-tuning-example](https://github.com/vmware-ai-labs/VMware-generative-ai-reference-architecture/tree/main/Examples/LLM-fine-tuning-example) directory to create a conda virtual environment with all the libraries used in this notebook.\n",
    "\n",
    "To use Llama2 from the Transformers library you need to get approval from Meta and HuggingFace. \n",
    "- Please follow these [instructions provided by HuggingFace](https://huggingface.co/meta-llama) before continuing.\n",
    "- Then [create a Hugging Face user access token](https://huggingface.co/docs/hub/security-tokens) to be able to download LLama 2 models from HF Hub.\n",
    "- Once you have created an access token, please log into the HuggingFace repository from the shell terminal using any of these options:\n",
    "    ```\n",
    "    # Using an interactive command\n",
    "    huggingface-cli login\n",
    "  \n",
    "    # or using an environment variable\n",
    "    huggingface-cli login --token $HUGGINGFACE_TOKEN\n",
    "    ```\n",
    "You can serve Llama-2-13B-chat on an A100 (40GB) GPU using vLLM by running the following command:\n",
    "`python -m vllm.entrypoints.openai.api_server --model=\"meta-llama/Llama-2-13b-chat-hf\"`\n",
    "\n",
    "Now, you should be all set to get started!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bce361dd40038f4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports section"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2425f6d14ab6c6b6"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.llms import VLLMOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T19:00:46.954682616Z",
     "start_time": "2023-10-31T19:00:43.467910853Z"
    }
   },
   "id": "8a59248939ae159e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generation parameters initialization and helper functions definitions\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edc63ede0269429a"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# URL where vLLM serves the Llama-2-13b-chat LLM\n",
    "INFERENCE_SRV_URL = \"http://localhost:8000/v1\"\n",
    "\n",
    "def setup_chat_llm(max_tokens=500, temperature=0.1, top_p=.9):\n",
    "    \"\"\"\n",
    "    Initializes the llm chat object that allows language chains get access\n",
    "    to the Llama 2 LLM service.\n",
    "    :param max_tokens: Max num. of tokens to generate\n",
    "    :param max_tokens: Max num. of tokens to generate.\n",
    "    :param temperature:  Determines how creative the model should be.\n",
    "    :param top_p: Cumulative probability threshold for selecting the next word\n",
    "    :return: the llm service callable object\n",
    "    \"\"\"\n",
    "    llm = VLLMOpenAI(\n",
    "        openai_api_key = \"EMPTY\",\n",
    "        openai_api_base = INFERENCE_SRV_URL,\n",
    "        model_name = \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "        max_tokens = max_tokens,\n",
    "        temperature = temperature,\n",
    "        top_p = top_p,\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "# >> Prompt template definitions for LLama 2 chat\n",
    "# We follow the prompt formatting guidelines from Meta's paper about Llama 2 chat.\n",
    "# For more info, see https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\n",
    "\n",
    "# System prompt template\n",
    "sys_prompt_template = (\n",
    "\"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful, and honest assistant.\n",
    "Always provide helpful, truthful, and safe answers. Safety must be your highest priority.\n",
    "Your answers must not contain harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in tone and nature.\n",
    "If an instruction doesn't make sense or is not coherent, inform the user instead of trying to answer.\n",
    "Don't provide false information if you don't know the answer to a question.\n",
    "<</SYS>>\n",
    "\"\"\")\n",
    "# Initialize the system (partial) prompt object for LangChain\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(sys_prompt_template)\n",
    "\n",
    "# Human (user) prompt section template definition\n",
    "human_template = \"{delimiter}{user_message} [/INST]\"\n",
    "# LangChain's human (partial) prompt message object initialization.\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# >> Prompt template initialization. Notice the placeholder for the \"chat_history\" key\n",
    "# that LangChain will use to give the LLM context about its dialog with the user.\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        system_message_prompt,\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        human_message_prompt,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# >> Initialize LangChain's ConversationBufferMemory used for conversation bookkeeping \n",
    "# The \"memory\" and \"input\" keys should match keys from the prompt. \n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",\n",
    "                                  input_key=\"user_message\",\n",
    "                                  return_messages=True)\n",
    "# Initialize the LLM service object\n",
    "llm = setup_chat_llm()\n",
    "\n",
    "# >>> Create an LLMChain out of the llm, memory and prompt objects previously created.\n",
    "# This chain is designed to run queries against LLMs\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=False,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "def respond(user_message, chatbot, conversation=conversation):\n",
    "    \"\"\"\n",
    "    Generates a completion for a new prompt using previous context (if any) using\n",
    "    a formatted prompt\n",
    "    :param user_message: New message from user properly formatted for Llama 2\n",
    "    :param conversation: Conversation history kept by LangChain's ConversationBufferMemory\n",
    "    :param chatbot: Gradio's chatbot memory. Not needed by LangChain\n",
    "    :return: The response to be returned to the chatbot UI\n",
    "    \"\"\"\n",
    "    # Clear memory in the event of a reset button press\n",
    "    if not len(chatbot):\n",
    "        conversation.memory.clear()\n",
    "    # Pull chat memory contents\n",
    "    conv_w_context = conversation.memory.dict()['chat_memory']['messages']\n",
    "\n",
    "    # Has the conversation already started?\n",
    "    if not conv_w_context:\n",
    "        delimiter = \"\" # If not, no need to complete the prompt template.\n",
    "    else:\n",
    "        delimiter = \"</s><s> [INST] \" # If it has, complete the prompt.\n",
    "\n",
    "    # Run the LLMChain \n",
    "    response = conversation.predict(delimiter=delimiter,\n",
    "                                    user_message=user_message)\n",
    "    # Store user's message and the LLLM responset in chatbot history\n",
    "    chatbot.append((user_message, response))\n",
    "    # Return the expanded dialog history\n",
    "    return \"\", chatbot"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T19:00:46.974779488Z",
     "start_time": "2023-10-31T19:00:46.962487008Z"
    }
   },
   "id": "e103af0eccd67f79"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": ""
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the Gradio client session\n",
    "with gr.Blocks() as demo:\n",
    "    # Set the size of the chat window\n",
    "    chatbot = gr.Chatbot(height=500, show_copy_button=True, interactive=True)\n",
    "    # Set the text box for users to enter text.\n",
    "    msg = gr.Textbox(label=\"Prompt\", interactive=True, show_copy_button=True)\n",
    "    # Expose 'submit' button\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    # Expose \"clear console\" button\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], \n",
    "                           value=\"Clear chat\")\n",
    "    # Submit the prompt completion request on a click event\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    # Submit the prompt completion request on an Enter key press event\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "\n",
    "# Close pre-existing gradio clients\n",
    "gr.close_all()\n",
    "\n",
    "# Launch Gradio demo on a new browser tab\n",
    "demo.launch(\n",
    "    inline=False,\n",
    "    inbrowser=True,\n",
    "    share=False,\n",
    "    show_error=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T19:00:47.929179799Z",
     "start_time": "2023-10-31T19:00:46.973851046Z"
    }
   },
   "id": "59b6d441cfb8db98"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test\n",
    "You can try the following dialog with the chatbot by introducing the prompts shown after `Human:` using the UI launched in your web browser. <br><br>\n",
    "\n",
    "### Example\n",
    "Human: <blockquote>Describe the four seasons in Earth's northern hemisphere</blockquote>\n",
    "AI: <blockquote>Hello! I'd be happy to help you with that! The four seasons in the northern hemisphere are: \n",
    "- Spring: This season usually starts in March and lasts until May. During this time, the weather starts to warm up, and plants begin to grow and bloom. It's a great time to go outside and enjoy nature!\n",
    "- Summer: Summer typically starts in June and ends in August. This is the warmest season of the year, with long days and plenty of sunshine. It's perfect for spending time outdoors, whether you're going to the beach, having a picnic, or just enjoying the sunshine.\n",
    "- Autumn (or Fall): Autumn starts in September and ends in November. During this season, the leaves on the trees start to change color and fall off, creating beautiful scenery. The weather can be a bit cooler, but it's still a great time to enjoy the outdoors.\n",
    "- Winter: Winter usually starts in December and ends in February. This is the coldest season of the year, with shorter days and less sunlight. It's a great time to cozy up indoors with a good book or a warm cup of tea.\n",
    "I hope that helps! Let me know if you have any other questions.</blockquote>\n",
    "\n",
    "Human: <blockquote>And what happens in the southern hemisphere during that time?</blockquote>\n",
    "AI: <blockquote>Hello! I'd be happy to help you with that! \n",
    "In the southern hemisphere, the seasons are reversed compared to the northern hemisphere. So, while it's winter in the north, it's summer in the south, and vice versa...</blockquote>\n",
    "\n",
    "Human: <blockquote>What about the Equator region of the planet? Are there any seasons? </blockquote>\n",
    "AI: <blockquote>AI:  Hello! I'd be happy to help you with that!\n",
    "The Equator region of the planet experiences a unique climate known as the equatorial climate. Since the Equator is located at zero degrees latitude, the sun's rays hit the region directly, resulting in high temperatures and high humidity throughout the year.</blockquote>\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "150b245ff59fcb9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fd01fd79ba5af9b1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
