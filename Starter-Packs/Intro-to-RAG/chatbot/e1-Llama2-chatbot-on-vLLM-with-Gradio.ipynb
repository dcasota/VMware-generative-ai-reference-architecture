{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Implementing a chatbot with Llama-2-13b-chat served by vLLM, through the Gradio UI\n",
    "- [**vLLM**](https://vllm.readthedocs.io/en/latest/) is a fast and easy-to-use library for LLM inference and serving.\n",
    "- [**Gradio**](https://www.gradio.app/) is a fast way to demo machine learning models with a friendly web interface.\n",
    "\n",
    "### Requirements\n",
    "Please follow the instructions provided by the `README.md` file from the \n",
    "[LLM-fine-tuning-example](https://github.com/vmware-ai-labs/VMware-generative-ai-reference-architecture/tree/main/Examples/LLM-fine-tuning-example) directory to create a conda virtual environment with all the libraries used in this notebook.\n",
    "\n",
    "To use Llama2 from the Transformers library you need to get approval from Meta and HuggingFace. \n",
    "- Please follow these [instructions provided by HuggingFace](https://huggingface.co/meta-llama) before continuing.\n",
    "- Then [create a Hugging Face user access token](https://huggingface.co/docs/hub/security-tokens) to be able to download LLama 2 models from HF Hub.\n",
    "- Once you have created an access token, please log into the HuugingFace repository from the shell terminal using any of these options:\n",
    "    ```\n",
    "    # Using an interactive command\n",
    "    huggingface-cli login\n",
    "  \n",
    "    # or using an environment variable\n",
    "    huggingface-cli login --token $HUGGINGFACE_TOKEN\n",
    "    ```\n",
    "You can serve Llama-2-13B-chat on an A100 (40GB) GPU using vLLM by running the following command:\n",
    "    `python -m vllm.entrypoints.openai.api_server --model=\"meta-llama/Llama-2-13b-chat-hf\"`\n",
    "\n",
    "Now, you should be all set to get started!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb8153645e20ff92"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports section"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5364d1ddbe8d13a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# >> We won't use OpenAI's LLMs but thanks to vLLM, \n",
    "# we can consume opensource LLMs via the OpenAI Python API\n",
    "import openai\n",
    "import gradio as gr"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generation parameters initialization and helper functions definitions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7f148a7f7673f2b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# >> Prompt template for LLama 2 chat\n",
    "# We follow the prompt formatting guidelines from Meta's paper about Llama 2 chat.\n",
    "# For more info, see https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\n",
    "\n",
    "P_TEMPLATE = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful, and honest assistant. \n",
    "Always provide helpful, truthful, and safe answers. Safety must be your highest priority.\n",
    "Your answers must not contain harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \n",
    "Please ensure that your responses are socially unbiased and positive in tone and nature.\n",
    "If an instruction doesn't make sense or is not coherent, inform the user instead of trying to answer. \n",
    "Don't provide false information if you don't know the answer to a question.\n",
    "<</SYS>>\n",
    "\n",
    "{user_message} [/INST]\"\"\"\n",
    "\n",
    "# >> Set the initialization parameter for the OpenAi API client .\n",
    "openai.api_key = \"EMPTY\" # Not using OpenAI service\n",
    "openai.api_base = \"http://localhost:8000/v1\" # Default vLLM serving \n",
    "models = openai.Model.list() # List models API\n",
    "model = models[\"data\"][0][\"id\"] # Get LLama 2 model name from the vLLM service\n",
    "\n",
    "# Query the OpenAI-compatible API from a vLLM server\n",
    "def generate(user_message, max_tokens=500, temperature=0.1, top_p=.9):\n",
    "    \"\"\"\n",
    "    Generates a prompt completion using an LLM service.\n",
    "    :param user_message: Formatted prompt to get completed by the LLM\n",
    "    :param max_tokens: Max num. of tokens to generate\n",
    "    :param temperature:  Determines how creative the model should be.\n",
    "    :param top_p: Cumulative probability threshold for selecting the next word.\n",
    "    :return: Prompt completion as a string\n",
    "    \"\"\"\n",
    "    completion = openai.Completion.create(\n",
    "        model=model, # vLLM service name for Llama 2\n",
    "        prompt=user_message.strip(), \n",
    "        echo=False, \n",
    "        n=1, # Get one result from greedy search\n",
    "        max_tokens=max_tokens, \n",
    "        stream=False, # Streaming complicates content moderation\n",
    "        repetition_penalty=1.1, # Prevent the LLm from repeating itself\n",
    "        temperature=temperature, \n",
    "        top_p=top_p, # Top-p is  used to exclude outputs with lower probabilities.\n",
    "    )\n",
    "    return completion[\"choices\"][0][\"text\"]\n",
    "    \n",
    "# Format new prompts including previous dialog history.\n",
    "def format_chat_prompt(new_message, conversation, prompt=P_TEMPLATE):\n",
    "    \"\"\"\n",
    "    Formats a new user's message to comply with LLama 2 chat prompt template:\n",
    "    \n",
    "   <s>[INST] <<SYS>>\n",
    "   {your_system_message}\n",
    "   <</SYS>>\n",
    "   \n",
    "   {user_message_1} [/INST] {model_reply_1}</s><s>[INST] {user_message_2} {model_reply_2}[/INST] ...\n",
    "    \n",
    "    :param new_message: New message from user in a conversation\n",
    "    :param conversation: The conversation history between a user and the LLM\n",
    "    :param prompt template to format\n",
    "    :return: A formatted prompt (conversation context included) \n",
    "    \"\"\"\n",
    "    if len(conversation) == 0:\n",
    "        # Use format for the first user message\n",
    "        return prompt.format(user_message=new_message)\n",
    "\n",
    "    for i, turn in enumerate(conversation): # Iterate over conversation history\n",
    "        user, assistant = turn # Gradio keeps interactions as a list of (user/assistant) tuples\n",
    "        if i:\n",
    "            # Formatting when there are multiple interactions already\n",
    "            prompt = prompt + f\"<s> [INST] {user} [/INST] {assistant} </s>\"\n",
    "        else:\n",
    "            # Formatting when there's one interaction already\n",
    "            prompt = prompt.format(user_message=user)\n",
    "            prompt = prompt + f\" {assistant} </s>\"\n",
    "    \n",
    "    return prompt + f\"[INST] {new_message} [/INST]\"\n",
    "\n",
    "def respond(message, conversation, max_tokens=500, temperature=0.1, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generates a completion for a new prompt using previous context (if any) using\n",
    "    a formatted prompt\n",
    "    :param message: New message from user properly formatted for Llama 2\n",
    "    :param conversation: Conversation history\n",
    "    :param max_tokens: Max num. of tokens to generate\n",
    "    :param temperature:  Determines how creative the model should be.\n",
    "    :param top_p: Determines how creative the model should be.\n",
    "    :return: The response to be returned to the chatbot UI\n",
    "    \"\"\"\n",
    "    formatted_prompt = format_chat_prompt(message, conversation)\n",
    "    bot_message = generate(formatted_prompt, max_tokens, temperature, top_p)\n",
    "    conversation.append((message, bot_message))\n",
    "    return \"\", conversation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f1c215928eafbb8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up the Gradio UI to expose the LLama 2 chatbot."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26e4d1ff4be30b46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set up the Gradio client session\n",
    "with gr.Blocks() as demo:\n",
    "    # Set the size of the chat window\n",
    "    chatbot = gr.Chatbot(height=500, show_copy_button=True, interactive=True)\n",
    "    # Set the text box for users to enter text.\n",
    "    msg = gr.Textbox(label=\"Prompt\", interactive=True, show_copy_button=True)\n",
    "    # Display nobs that control LLM generation options \n",
    "    with gr.Accordion(label=\"Generation Options\",open=False):\n",
    "        temp = gr.Slider(label=\"Temperature\", minimum=0.1, maximum=1, value=0.1, step=0.1)\n",
    "        top_p = gr.Slider(label=\"Top_p\", minimum=0.1, maximum=1, value=0.9, step=0.1)\n",
    "        max_tokens = gr.Slider(label=\"Max # of Tokens\", minimum=128, maximum=1024, value=512, step=32)\n",
    "    # Expose 'submit' button\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    # Expose \"clear console\" button\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear Console\")\n",
    "    # Submit the prompt completion request on a click event\n",
    "    btn.click(respond, inputs=[msg, chatbot, max_tokens, temp], outputs=[msg, chatbot])\n",
    "    # Submit the prompt completion request on an Enter key press event\n",
    "    msg.submit(respond, inputs=[msg, chatbot, max_tokens, temp], outputs=[msg, chatbot])\n",
    "\n",
    "# Close pre-existing gradio clients\n",
    "gr.close_all()\n",
    "\n",
    "# Launch Gradio demo on a new browser tab\n",
    "demo.launch(\n",
    "    inline=False,\n",
    "    inbrowser=True,\n",
    "    share=False,\n",
    "    show_error=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7823041501d4e833"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test\n",
    "You can try the following dialog with the chatbot by introducing the prompts shown after `Human:` using the UI launched in your web browser. <br><br>\n",
    "\n",
    "### Example\n",
    "Human: <blockquote>Describe the four seasons in Earth's northern hemisphere</blockquote>\n",
    "AI: <blockquote>Hello! I'd be happy to help you with that! The four seasons in the northern hemisphere are:\n",
    "- Spring: This season usually starts in March and lasts until May. During this time, the weather starts to warm up, and plants begin to grow and bloom. It's a great time to go outside and enjoy nature!\n",
    "- Summer: Summer typically starts in June and ends in August. This is the warmest season of the year, with long days and plenty of sunshine. It's perfect for spending time outdoors, whether you're going to the beach, having a picnic, or just enjoying the sunshine.\n",
    "- Autumn (or Fall): Autumn starts in September and ends in November. During this season, the leaves on the trees start to change color and fall off, creating beautiful scenery. The weather can be a bit cooler, but it's still a great time to enjoy the outdoors.\n",
    "- Winter: Winter usually starts in December and ends in February. This is the coldest season of the year, with shorter days and less sunlight. It's a great time to cozy up indoors with a good book or a warm cup of tea.\n",
    "I hope that helps! Let me know if you have any other questions.</blockquote>\n",
    "\n",
    "Human: <blockquote>And what happens in the southern hemisphere during that time?</blockquote>\n",
    "AI: <blockquote>Hello! I'd be happy to help you with that!\n",
    "In the southern hemisphere, the seasons are reversed compared to the northern hemisphere. So, while it's winter in the north, it's summer in the south, and vice versa...</blockquote>\n",
    "\n",
    "Human: <blockquote>What about the Equator region of the planet? Are there any seasons? </blockquote>\n",
    "AI: <blockquote>AI:  Hello! I'd be happy to help you with that!\n",
    "The Equator region of the planet experiences a unique climate known as the equatorial climate. Since the Equator is located at zero degrees latitude, the sun's rays hit the region directly, resulting in high temperatures and high humidity throughout the year.</blockquote>\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c31eec06c89b364"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "93002975a032d3e3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
