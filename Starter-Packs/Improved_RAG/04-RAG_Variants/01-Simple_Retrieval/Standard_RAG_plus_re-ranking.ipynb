{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Simple RAG (+ Re-ranker) from a PGVector Store"
   ],
   "metadata": {},
   "id": "21f9146a-c6f3-4a78-b3fb-0d262492e87c"
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "# Start timer to time the notebook execution\n",
    "start = time.time()\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import make_url\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "from llama_index.core.indices.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "import sys\n",
    "utils_path = \"../../utils\"\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "\n",
    "from helpers import generate_responses_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:31:32.533346Z",
     "start_time": "2024-04-09T00:31:27.612506Z"
    }
   },
   "id": "cb14fd5b562cb2ad",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# Path of the evaluation test set with Q/A pairs\n",
    "TEST_SET = (\"../../05-RAG_Dataset_Generation/LlamaIndex_generation/\"\n",
    "            \"qa_datasets/Mixtral-8x7B-Instruct-v0.1/NASA_history_qa_only.csv\")\n",
    "\n",
    "# vLLM service settings\n",
    "LLM_MODEL = \"HuggingFaceH4/zephyr-7b-alpha\" # You may replace this with a different model\n",
    "LLM_API_BASE = \"http://localhost:8010/v1\" # The URL vLLM service is accessible from.\n",
    "LLM_API_KEY = \"NO_KEY\" # By default, vLLM does not require a key.\n",
    "GEN_TEMP=0.1 # Generation temperature\n",
    "MAX_TOKENS=512 # Max tokens the LLM should generate \n",
    "REP_PENALTY=1.03 # Word repetition penalty at generation time\n",
    "\n",
    "# LLamaIndex LLM provider\n",
    "Settings.llm = OpenAILike(\n",
    "    model=LLM_MODEL,\n",
    "    api_key=LLM_API_KEY,\n",
    "    api_base=LLM_API_BASE,\n",
    "    temperature=GEN_TEMP,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    repetition_penalty=REP_PENALTY,\n",
    ")\n",
    "\n",
    "# Retriever and re_ranker settings\n",
    "SIMIL_TOP_K = 8 # Retrieve TOP_K most similar docs from the PGVector store\n",
    "RERANK_TOP_N = 5 # Rerank and pick the 5 most similar docs\n",
    "RERANK_MODEL = \"BAAI/bge-reranker-base\" # Re-ranking model\n",
    "\n",
    "# LLamaIndex embedding model\n",
    "EMB_MODEL=\"BAAI/bge-base-en-v1.5\" # For better results you can use the \"large\" variant\n",
    "DEVICE=\"cuda:0\" # If running out of GPU RAM, switch to \"cpu\" (although slower)\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=EMB_MODEL,\n",
    "    device=DEVICE\n",
    ")\n",
    "EMBEDDING_SIZE = len(Settings.embed_model.get_text_embedding(\"hi\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:31:35.623495Z",
     "start_time": "2024-04-09T00:31:32.537213Z"
    }
   },
   "id": "3eb2c391167f0fe1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "# PGVector DB params as defines in the reference Docker compose file\n",
    "## available from the \"../PGvector\" directory\n",
    "DB_PORT = 5432\n",
    "DB_USER = \"demouser\"\n",
    "DB_PASSWD = \"demopasswd\"\n",
    "DEFAULT_DB = \"postgres\"\n",
    "DB_NAME = \"vectordb\"\n",
    "DB_HOST = \"localhost\"\n",
    "TABLE_NAME = \"NASA_HISTORY_BOOKS\"\n",
    "connection_string = f\"postgresql://{DB_USER}:{DB_PASSWD}@{DB_HOST}:{DB_PORT}/{DEFAULT_DB}\"\n",
    "url = make_url(connection_string)\n",
    "\n",
    "# Open the connection to the Vector Store\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=DB_NAME,\n",
    "    host=url.host,\n",
    "    password=url.password,\n",
    "    port=url.port,\n",
    "    user=url.username,\n",
    "    table_name=TABLE_NAME,\n",
    "    embed_dim=EMBEDDING_SIZE, # embedding model dimension\n",
    "    cache_ok=True,\n",
    "    hybrid_search=True,\n",
    ")\n",
    "\n",
    "# Initialize the index object\n",
    "index = VectorStoreIndex.from_vector_store(vector_store)\n",
    "\n",
    "# Initialize the re-ranker of retrieved chunks\n",
    "re_ranker = SentenceTransformerRerank(\n",
    "    top_n=RERANK_TOP_N,\n",
    "    model=RERANK_MODEL,\n",
    ")\n",
    "\n",
    "# Set the index as query engine\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=SIMIL_TOP_K,\n",
    "    node_postprocessors=[re_ranker],\n",
    "    vector_store_kwargs={\"hnsw_ef_search\": 256},\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:31:37.543552Z",
     "start_time": "2024-04-09T00:31:35.626731Z"
    }
   },
   "id": "e099027279772442",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.85 s, sys: 2.08 s, total: 10.9 s\n",
      "Wall time: 1.91 s\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "# Load a test set to run inference on multiple Q/A pairs\n",
    "test_set_df = pd.read_csv(\n",
    "    filepath_or_buffer=TEST_SET,\n",
    "    usecols=['query', 'reference_answer'],\n",
    ")\n",
    "\n",
    "# Run inference on the Q/A pairs and keep the responses\n",
    "# for future comparison vs.other RAG approaches.\n",
    "responses = generate_responses_dict(\n",
    "    query_engine=query_engine,\n",
    "    test_set_df=test_set_df\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:53:44.290984Z",
     "start_time": "2024-04-09T00:31:37.545152Z"
    }
   },
   "id": "ee93a30b32392fdf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7eb2555cd35d4c13ac1b3c58e881b831"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.6 s, sys: 2.87 s, total: 39.5 s\n",
      "Wall time: 22min 6s\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert the responses into a Pandas data frame\n",
    "responses_df = pd.DataFrame.from_dict(responses)\n",
    "\n",
    "# Serialize the inference results dataframe\n",
    "file_name = (f\"Standard_RAG_{LLM_MODEL.split('/')[1]}\"\n",
    "             f\"_{EMB_MODEL.split('/')[1]}.csv\")\n",
    "\n",
    "responses_df.to_csv(\n",
    "    path_or_buf=file_name,\n",
    "    index=False,\n",
    ")\n",
    "stop = time.time()\n",
    "print(f\"Notebook execution time: {(stop-start)/60:.1f} minutes\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:53:44.378234Z",
     "start_time": "2024-04-09T00:53:44.293749Z"
    }
   },
   "id": "830ec3cba9fb599a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook execution time: 22.3 minutes\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a98a925dfc8b393a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
