{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8cbe152-de29-4240-8e13-f74dc146a658",
   "metadata": {
    "id": "c8cbe152-de29-4240-8e13-f74dc146a658"
   },
   "source": [
    "# Notebook 1: PDF Documents Ingestion Pipeline into PGVector\n",
    "The notebook provides a reference implementation of a __document ingestion pipeline__ that creates a __knowledgebase from documents belonging to a specific domain__. The  \n",
    "documents will be encoded as high dimensional vectors and these will get stored in a __vector DB__ powered by __PGVector (PostgreSQL)__. This vector store will provide  \n",
    "the __retrieval engine__  to __augment and ground the generation process of LLMs__ when __answering questions__ about the __knowledge domain__ bounded by the ingested documents.\n",
    "\n",
    "![Ingestion Pipeline Process Flow](img/PGVector_store_population.png)\n",
    "\n",
    "## Notebook Details and Preliminary Instructions\n",
    "- The knowledge base consists of [__10 NASA history books__](https://www.nasa.gov/ebooks/). The RAG pipelines we build in this series of notebooks has the mission to __ground an LLM to make it answer questions about the   \n",
    "content of these books__ without making things up or using its pre-training data as knowledge base to generate answers.\n",
    "- The ingestion pipeline is powered by __LlamaIndex (tested on v0.10.26)__\n",
    "- The embedding model is [__BAAI/bge-base-en-v1.5__](https://huggingface.co/BAAI/bge-base-en-v1.5) which ranks high at the [__MTEB Leaderboard__](https://huggingface.co/spaces/mteb/leaderboard). This family of models can get [__CPU-optimized__](https://huggingface.co/blog/intel-fast-embedding) to save GPU memory.\n",
    "- The LLM used to execute generation tasks in the RAG pipeline is [__HuggingFaceH4/zephyr-7b-alpha__](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha). This  \n",
    "model is [__compatible with LlamaIndex__](https://docs.llamaindex.ai/en/latest/module_guides/models/llms/) (notice it is the open-source LLM ticking more checkboxes)  \n",
    "and in our experiments showed better quality generations than other models of similar size (and even bigger sizes). \n",
    "\n",
    "### BEFORE YOU START:\n",
    "- The LLM runs on [__vLLM__](https://github.com/vllm-project/vllm) which is one (if not the most) popular open-source LLM inference engine. \n",
    "    -- To run __Zephyr-7b-alpha__ on __vLLM__ in __OpenAI-compatible mode__, make sure you have an __A100 (40GB) GPU__ available at the OS-level and CUDA 12.1 installed.\n",
    "    -- Then you need to run the following commands to make the LLM service available from `http://localhost:8010/v1`:  \n",
    "  ```\n",
    "        # (Optional) Create a new conda environment.\n",
    "        conda create -n vllm-env python=3.9 -y\n",
    "        conda activate vllm-env\n",
    "\n",
    "        # Install vLLM with CUDA 12.1.\n",
    "        pip install vllm\n",
    "        \n",
    "        # Serve the zephyr-7b-alpha LLM\n",
    "        python -m vllm.entrypoints.openai.api_server --model HuggingFaceH4/zephyr-7b-alpha --port 8010 --enforce-eager\n",
    "  ```\n",
    "\n",
    "- The vector store is implemented using the __PGVector extension of PostgreSQL__ (v12). For the purposes of this demo, __please go to the PGVector directory (`../PGVector`) in this  \n",
    "repository and execute the `run_pgvector.sh` script to pull and launch a PostgreSQL + PGVector Docker container.__ Once up and running, the DB engine will be available  \n",
    "from `localhost:5432`\n",
    "\n",
    "- Finally, it is necessary to create a new Conda environment with the required packages to run the Python scripts. Here the steps to do this:\n",
    "  ```\n",
    "    # From a shell terminal, go to the \"utils\" directory. Then run the following command:\n",
    "    conda env create -f adv_rag.yml\n",
    "\n",
    "    # Wait several minutes until a the \"adv_rag\" conda environment gets created.\n",
    "    # Next, activate the new Conda env\n",
    "    conda activate adv_rag.yml\n",
    "  ```\n",
    "- Make sure you run the Python scripts from the Improved RAG Starter Pack always from the __adv_rag__ Conda environment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports Section"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82e3fed87148b4ef"
  },
  {
   "cell_type": "code",
   "id": "44f32ec9eeb84e66",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:26:01.138365Z",
     "start_time": "2024-04-09T00:25:55.852102Z"
    }
   },
   "source": [
    "# General purpose imports\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import make_url\n",
    "from pprint import pprint\n",
    "\n",
    "# LlamaIndex imports\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "# Imports from this repo\n",
    "import sys\n",
    "utils_path = \"../utils\"\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "\n",
    "from helpers import (\n",
    "    get_indices_with_nulls, \n",
    "    remove_elements,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Global Config Setup \n",
    "- Here we define most of the RAG pipeline parameters and LlamaIndex defaults.\n",
    "- The goal is to provide a single control cell to define the notebook's execution."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b61cdfb6594b23fd"
  },
  {
   "cell_type": "code",
   "source": [
    "# vLLM service settings\n",
    "LLM_MODEL = \"HuggingFaceH4/zephyr-7b-alpha\" # You may replace this with a different model\n",
    "LLM_API_BASE = \"http://localhost:8010/v1\" # The URL vLLM service is accessible from.\n",
    "LLM_API_KEY = \"NO_KEY\" # By default, vLLM does not require a key.\n",
    "GEN_TEMP=0.1 # Generation temperature\n",
    "MAX_TOKENS=512 # Max tokens the LLM should generate \n",
    "REP_PENALTY=1.03 # Word repetition penalty at generation time\n",
    "\n",
    "# LLamaIndex LLM provider\n",
    "Settings.llm = OpenAILike(\n",
    "    model=LLM_MODEL,\n",
    "    api_key=LLM_API_KEY,\n",
    "    api_base=LLM_API_BASE,\n",
    "    temperature=GEN_TEMP,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    repetition_penalty=REP_PENALTY,\n",
    ")\n",
    "\n",
    "# LLamaIndex embedding model\n",
    "EMB_MODEL=\"BAAI/bge-base-en-v1.5\" # For better results you can use the \"large\" variant\n",
    "DEVICE=\"cuda:0\" # If running out of GPU RAM, switch to \"cpu\" (although slower)\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=EMB_MODEL,\n",
    "    device=DEVICE\n",
    ")\n",
    "EMBEDDING_SIZE = len(Settings.embed_model.get_text_embedding(\"hi\"))\n",
    "\n",
    "# PGVector DB params as defines in the reference Docker compose file\n",
    "## available from the \"../PGvector\" directory\n",
    "DB_PORT = 5432\n",
    "DB_USER = \"demouser\"\n",
    "DB_PASSWD = \"demopasswd\"\n",
    "DEFAULT_DB = \"postgres\"\n",
    "DB_NAME = \"vectordb\"\n",
    "DB_HOST = \"localhost\"\n",
    "TABLE_NAME = \"NASA_HISTORY_BOOKS\"\n",
    "\n",
    "# Ingestion pipeline settings\n",
    "NUM_WORKERS = 4 # Num. of parallel workers for the ingestion process\n",
    "CHUNK_SIZE = 1024 # Num. of words per chunk \n",
    "PDF_FILES_PATH = \"../02-KB-Documents/NASA\" # The directory to read PDF files from\n",
    "MIN_DOC_LENGTH = 40 # Minimum number of words allowed per doc"
   ],
   "metadata": {
    "id": "e0963707-6ebe-4441-a363-1bfb48ce9df3",
    "ExecuteTime": {
     "end_time": "2024-04-09T00:27:02.658402Z",
     "start_time": "2024-04-09T00:27:01.286800Z"
    }
   },
   "id": "e0963707-6ebe-4441-a363-1bfb48ce9df3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "92686bb0-85ed-4bb3-99eb-f5fc6c100787",
   "metadata": {
    "id": "92686bb0-85ed-4bb3-99eb-f5fc6c100787"
   },
   "source": [
    "## PDF Documents Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# Lamda function to add the file name as metadata at loading time\n",
    "filename_fn = lambda filename: {\"file_name\": filename.split(\"/\")[-1]}\n",
    "\n",
    "# >> PDF Document Parsing\n",
    "# The PyMuPDFReader takes ~ 1/20 the time it takes to the default reader to ingest the PDF files\n",
    "# Note: PyMuPDFReader creates a document object per page in a PDF document.\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir=PDF_FILES_PATH,\n",
    "    required_exts=[\".pdf\"],\n",
    "    file_extractor={\".pdf\":PyMuPDFReader()},\n",
    "    file_metadata=filename_fn,\n",
    "    num_files_limit=10,\n",
    ")\n",
    "documents = reader.load_data()\n",
    "\n",
    "# Filter out documents with null (`\\x00') characters which are incompatible with PGVector.\n",
    "bad_docs = get_indices_with_nulls(documents)\n",
    "documents = remove_elements(documents, bad_docs)\n",
    "print(f\"Loaded {len(documents)} pages\")\n",
    "\n",
    "# Display one document as example\n",
    "example_doc = 10\n",
    "print(f\" >> TEXT FROM DOCUMENT #{example_doc}:\\n\", documents[example_doc], \"\\n\")\n",
    "print(f\" >> DOCUMENT {example_doc} METADATA:\")\n",
    "pprint(documents[example_doc].metadata, depth=1, indent=4, width=100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:27:20.749243Z",
     "start_time": "2024-04-09T00:27:02.661140Z"
    }
   },
   "id": "f674caa1b8cec1e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3855 pages\n",
      " >> TEXT FROM DOCUMENT #10:\n",
      " Doc ID: 6baac775-12b5-469c-ae11-5c6c424dca17\n",
      "Text: 1 INTRODUCTION NASA’s Solar System Exploration Paradigm:  The\n",
      "First 50 Years and a  Look at the Next 50 James L. Green and Kristen\n",
      "J. Erickson A FTER MANY FAILURES to get to the Moon and to the planets\n",
      "beyond,  Mariner 2 successfully flew by Venus in December 1962. This\n",
      "historic  mission began a spectacular era of solar system exploration\n",
      "for NA... \n",
      "\n",
      " >> DOCUMENT 10 METADATA:\n",
      "{   'file_name': '50-years-of-solar-system-exploration_tagged.pdf',\n",
      "    'file_path': '/home/vmuser/RAG/Improved_RAG_Starter_Pack/03-Document_ingestion/../02-KB-Documents/NASA/50-years-of-solar-system-exploration_tagged.pdf',\n",
      "    'source': '11',\n",
      "    'total_pages': 364}\n",
      "CPU times: user 17.6 s, sys: 472 ms, total: 18.1 s\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Analysis Section\n",
    "- This section is useful to get a better understanding about th text data and its possible effects on LLM inference timeouts. It turns \n",
    "out that documents with few pages (<10) had too few or no words at all might make the LLM inference service to fail."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a15efa854825b5fb"
  },
  {
   "cell_type": "code",
   "source": [
    "# Helper dict to form a dataframe\n",
    "docs_metrics = {\n",
    "    'words': [],\n",
    "    'text': [],\n",
    "}\n",
    "for doc in documents:\n",
    "    docs_metrics['words'].append(len(doc.text.split()))\n",
    "    docs_metrics['text'].append(doc.text)\n",
    "docs_metrics = pd.DataFrame.from_dict(docs_metrics)\n",
    "\n",
    "# Get statistics about number of words in documents\n",
    "## Notice the presence of docs with 0 words\n",
    "print(\"Words per page statistics\")\n",
    "display(docs_metrics.words.describe())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:27:20.862910Z",
     "start_time": "2024-04-09T00:27:20.750775Z"
    }
   },
   "id": "40000930e2ef4b1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words per page statistics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    3855.000000\n",
       "mean      383.659403\n",
       "std       156.257311\n",
       "min         0.000000\n",
       "25%       304.000000\n",
       "50%       412.000000\n",
       "75%       474.000000\n",
       "max      2775.000000\n",
       "Name: words, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "# Display pages with too few words (length < MIN_DOC_LENGTH)\n",
    "print(f\">> Filter out documents with < {MIN_DOC_LENGTH} words\")\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "display(docs_metrics[docs_metrics.words<MIN_DOC_LENGTH])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:27:20.879523Z",
     "start_time": "2024-04-09T00:27:20.865506Z"
    }
   },
   "id": "dfa9a7cfe1f34b01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Filter out documents with < 40 words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      words  \\\n",
       "0        11   \n",
       "2         6   \n",
       "3         0   \n",
       "4        29   \n",
       "23        0   \n",
       "...     ...   \n",
       "3326     27   \n",
       "3423     29   \n",
       "3424     23   \n",
       "3843      0   \n",
       "3854     31   \n",
       "\n",
       "                                                                                                                                                       text  \n",
       "0                                                         HISTORICAL \\nPERSPECTIVES\\n \\n50 YEARS OF \\n SOLAR SYSTEM \\nEXPLORATION\\nLINDA BILLINGS, EDITOR\\n  \n",
       "2                                                                                                                 50 YEARS OF \\nSOLAR SYSTEM\\nEXPLORATION\\n  \n",
       "3                                                                                                                                                            \n",
       "4     50 YEARS OF \\nSOLAR SYSTEM\\nEXPLORATION \\nHISTORICAL PERSPECTIVES\\nEdited by \\nLINDA BILLINGS\\nNational Aeronautics and Space Administration\\nOffi...  \n",
       "23                                                                                                                                                           \n",
       "...                                                                                                                                                     ...  \n",
       "3326  383\\nDocument 5-25 (a–b)Figure 1. Schematics of Langley tank models 203, 213, 214, and 224.\\nFIGURE 1. Lines of Langley tank models 203, 213, 214,...  \n",
       "3423  The Wind and Beyond, Volume III\\n480Fig 1. Graph showing curves of characteristic coefficients for standing thrust and power of 2-blade uniform ge...  \n",
       "3424  481\\nDocument 5-31Fig 2. Graph showing curves of characteristic coefficients for standing thrust and power of 2-blade now warped propeller F2 A1 S...  \n",
       "3843                                                                                                                                                         \n",
       "3854  W ind an d Beyo\\nn d  e b\\nook barcode. ISBN 9781626830738\\nThe NASA History Series\\nNational Aeronautics and Space Administration\\nNASA History D...  \n",
       "\n",
       "[163 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>HISTORICAL \\nPERSPECTIVES\\n \\n50 YEARS OF \\n SOLAR SYSTEM \\nEXPLORATION\\nLINDA BILLINGS, EDITOR\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>50 YEARS OF \\nSOLAR SYSTEM\\nEXPLORATION\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>50 YEARS OF \\nSOLAR SYSTEM\\nEXPLORATION \\nHISTORICAL PERSPECTIVES\\nEdited by \\nLINDA BILLINGS\\nNational Aeronautics and Space Administration\\nOffi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3326</th>\n",
       "      <td>27</td>\n",
       "      <td>383\\nDocument 5-25 (a–b)Figure 1. Schematics of Langley tank models 203, 213, 214, and 224.\\nFIGURE 1. Lines of Langley tank models 203, 213, 214,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3423</th>\n",
       "      <td>29</td>\n",
       "      <td>The Wind and Beyond, Volume III\\n480Fig 1. Graph showing curves of characteristic coefficients for standing thrust and power of 2-blade uniform ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424</th>\n",
       "      <td>23</td>\n",
       "      <td>481\\nDocument 5-31Fig 2. Graph showing curves of characteristic coefficients for standing thrust and power of 2-blade now warped propeller F2 A1 S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3843</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3854</th>\n",
       "      <td>31</td>\n",
       "      <td>W ind an d Beyo\\nn d  e b\\nook barcode. ISBN 9781626830738\\nThe NASA History Series\\nNational Aeronautics and Space Administration\\nNASA History D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "# Remove \"too small\" pages from the corpus\n",
    "short_docs = docs_metrics.words[docs_metrics.words<MIN_DOC_LENGTH].index.to_list()\n",
    "print(f\">> Removing {len(short_docs)} pages from the corpus\")\n",
    "documents = remove_elements(documents, short_docs)\n",
    "print(f\" > New pages list size: {len(documents)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:27:20.898469Z",
     "start_time": "2024-04-09T00:27:20.880862Z"
    }
   },
   "id": "14339682da7093bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Removing 163 pages from the corpus\n",
      " > New pages list size: 3692\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "5b00be91-22ea-403c-b9c4-cd030b7e6c09",
   "metadata": {
    "id": "5b00be91-22ea-403c-b9c4-cd030b7e6c09"
   },
   "source": [
    "## Setup the Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "id": "1089adee-bc8a-457f-8d96-113435923d10",
   "metadata": {
    "id": "1089adee-bc8a-457f-8d96-113435923d10",
    "ExecuteTime": {
     "end_time": "2024-04-09T00:27:21.161090Z",
     "start_time": "2024-04-09T00:27:20.899785Z"
    }
   },
   "source": [
    "# Create the pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        # Split docs with preference to complete sentences\n",
    "        SentenceSplitter(\n",
    "            chunk_size=CHUNK_SIZE,\n",
    "            chunk_overlap=20\n",
    "        ),\n",
    "        # Generate embeddings for document splits\n",
    "        Settings.embed_model,\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "1937fbaa-0cef-494d-b3e1-a5ff268fd8d2",
   "metadata": {
    "id": "1937fbaa-0cef-494d-b3e1-a5ff268fd8d2"
   },
   "source": [
    "## Ingestion Pipeline Parallel Execution\n",
    "- Setting `num_workers` to a value greater than 1 will invoke parallel execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# Run the ingestion pipeline\n",
    "nodes = pipeline.run(\n",
    "    show_progress=True,\n",
    "    documents=documents,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "print(f\">> Created {len(nodes)} nodes.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:28:10.451445Z",
     "start_time": "2024-04-09T00:27:21.162853Z"
    }
   },
   "id": "f70e7e609c757217",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Created 3893 nodes.\n",
      "CPU times: user 4.74 s, sys: 560 ms, total: 5.3 s\n",
      "Wall time: 49.3 s\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "# Show the aspect of LlamaIndex nodes and their metadata.\n",
    "print(\">> LlamaIndex node's metadata sample:\\n\")\n",
    "# print(\" > Questions this node can answer:\")\n",
    "print(nodes[0].to_dict()['metadata'], \"\\n\")\n",
    "print(\">> LlamaIndex node's text:\\n\", nodes[0].text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:28:10.466218Z",
     "start_time": "2024-04-09T00:28:10.456344Z"
    }
   },
   "id": "4ef40a0334af2540",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> LlamaIndex node's metadata sample:\n",
      "\n",
      "{'file_name': '50-years-of-solar-system-exploration_tagged.pdf', 'total_pages': 364, 'file_path': '/home/vmuser/RAG/Improved_RAG_Starter_Pack/03-Document_ingestion/../02-KB-Documents/NASA/50-years-of-solar-system-exploration_tagged.pdf', 'source': '2'} \n",
      "\n",
      ">> LlamaIndex node's text:\n",
      " HISTORICAL PERSPECTIVES\n",
      "NASA’S FIRST SUCCESSFUL MISSION to another planet, \n",
      "Mariner 2 to Venus in 1962, marked the begin­\n",
      "ning of what NASA Chief Scientist Jim Green \n",
      "describes in this volume as “a spectacular era” \n",
      "of solar system exploration. In its first 50 years \n",
      "of planetary exploration, NASA sent spacecraft \n",
      "to fly by, orbit, land on, or rove on every planet \n",
      "in our solar system, as well as Earth’s Moon and \n",
      "several moons of other planets. Pluto, reclassi­\n",
      "fied as a dwarf planet in 2006, was visited by \n",
      "the New Horizons spacecraft in 2015.\n",
      "What began as an endeavor of two \n",
      "nations—the United States and the former \n",
      "Soviet Union—has become a multinational \n",
      "enterprise, with a growing number of space \n",
      "agencies worldwide building and launching plan­\n",
      "etary exploration missions—sometimes alone, \n",
      "sometimes together. \n",
      "In this volume, a diverse array of schol­\n",
      "ars addresses the science, technology, policy, \n",
      "and politics of planetary exploration. This vol­\n",
      "ume offers a collection of in-depth studies of \n",
      "important projects, decisions, and milestones \n",
      "of this era.\n",
      "It is not possible to foresee what the next \n",
      "50 years of NASA’s planetary exploration pro­\n",
      "gram will reveal. However, the 2020s are already \n",
      "looking promising. Planetary missions in recent \n",
      "years have focused more and more on explor­\n",
      "ing potentially habitable environments in our \n",
      "solar system and developing a more in-depth \n",
      "understanding of the evolution of planetary \n",
      "environments. Upcoming missions will continue \n",
      "to do so. In 2020, NASA launched its Mars 2020 \n",
      "rover, Perseverance, and in 2022 the European \n",
      "Space Agency will launch its Exomars rover. \n",
      "The Lucy and Psyche asteroid missions are set \n",
      "to launch in this decade, as are the Dragonfly \n",
      " \n",
      "50 YEARS OF \n",
      " SOLAR SYSTEM \n",
      "EXPLORATION\n",
      "FRONT COVER: Solar system exploration illustration. Credit: NASA/\n",
      "Jenny Mottar\n",
      "BACK COVER: Mariner 2 spacecraft. Credit: National Air and Space \n",
      "Museum\n",
      "mission to Titan and the Europa Clipper mission \n",
      "to the Jovian moon. Planetary defense is a new \n",
      "addition to NASA’s planetary portfolio, and the \n",
      "Agency’s Double Asteroid Redirection Test—its \n",
      "first planetary defense mission—is due to launch \n",
      "in 2021. NASA plans to develop a Near Earth \n",
      "Object Surveillance Mission this decade as well. \n",
      "It is safe to say that by 2062, our understand­\n",
      "ing of our solar system will be radically different \n",
      "than it is today. And we will look forward to it.\n",
      "ABOUT THE EDITOR: Linda Billings is a consul­\n",
      "tant to NASA’s Astrobiology Program and \n",
      "Planetary Defense Coordination Office in the \n",
      "Planetary Science Division of the Science \n",
      "Mission Directorate at NASA Headquarters in \n",
      "Washington, DC. She earned her Ph.D. in mass \n",
      "communication from Indiana University. Her \n",
      "research interests include science and risk com­\n",
      "munication, social studies of science, the his­\n",
      "tory of space science and exploration, and the \n",
      "rhetoric of science and space. She has contrib­\n",
      "uted chapters to several edited volumes, includ­\n",
      "ing First Contact: The Search for Extraterrestrial \n",
      "Intelligence (New American Library, 1990); \n",
      "Societal Impacts of Spaceflight (NASA History \n",
      "Division, 2007), Remembering the Space Age \n",
      "(NASA History Division, 2008); NASA’s First 50 \n",
      "Years: Historical Perspectives (NASA History \n",
      "Division, 2010); Space Shuttle Legacy: How \n",
      "We Did It and What We Learned (AIAA, 2013); \n",
      "The Impact of Discovering Life Beyond Earth \n",
      "(Cambridge University Press, 2015); and Social \n",
      "and Conceptual Issues in Astrobiology (Oxford \n",
      "University Press, 2020). She also has published \n",
      "papers and opinion pieces in Space Policy; Acta \n",
      "Astronautica; Scientific American; the Bulletin of \n",
      "Science, Technology, and Society; Space News; \n",
      "and Advances in Space Research. She was \n",
      "elected a fellow of the American Association \n",
      "for the Advancement of Science in 2009. She \n",
      "received an outstanding achievement award \n",
      "from Women in Aerospace (WIA) in 1992 and a \n",
      "lifetime achievement award from WIA in 2009.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the PGVector Store from an existing PostgresSQL DB"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "516e7498a0ad598f"
  },
  {
   "cell_type": "code",
   "source": [
    "# Connect to the PostgreSQL engine ans initialize de DB to serve as vector/document store.\n",
    "connection_string = f\"postgresql://{DB_USER}:{DB_PASSWD}@{DB_HOST}:{DB_PORT}/{DEFAULT_DB}\"\n",
    "conn = psycopg2.connect(connection_string)\n",
    "conn.autocommit = True\n",
    "with conn.cursor() as c:\n",
    "    c.execute(f\"DROP DATABASE IF EXISTS {DB_NAME}\")\n",
    "    c.execute(f\"CREATE DATABASE {DB_NAME}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:28:10.578383Z",
     "start_time": "2024-04-09T00:28:10.468846Z"
    }
   },
   "id": "90549c05a99c9ce6",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# Create a url object to store DB connection parameters\n",
    "url = make_url(connection_string)\n",
    "\n",
    "# Connect to the PGVector extension\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=DB_NAME,\n",
    "    host=url.host,\n",
    "    password=url.password,\n",
    "    port=url.port,\n",
    "    user=url.username,\n",
    "    table_name=TABLE_NAME,\n",
    "    embed_dim=EMBEDDING_SIZE, # embedding model dimension\n",
    "    cache_ok=True,\n",
    "    hybrid_search=True, # retrieve nodes based on vector values and keywords\n",
    ")\n",
    "\n",
    "# LlamaIndex persistence object backed by the PGVector connection\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# LlamaIndex index population (nodes -> embeddings -> vector store)\n",
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    storage_context=storage_context,\n",
    "    show_progress=True,\n",
    "    transformations=None,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:28:24.183653Z",
     "start_time": "2024-04-09T00:28:10.583463Z"
    }
   },
   "id": "404e852d43adbb5b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating embeddings: 0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "689d98bc54114c0487c4dcae1f1da180"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating embeddings: 0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b48bbd6c3ee04947aea121e23432c04d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.84 s, sys: 223 ms, total: 8.07 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RAG Pipeline Test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e061a41d60a4fd16"
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# Use the index as a query engine to give the LLM the required context\n",
    "## to answer questions about the NASA history knowledge domain.\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    vector_store_kwargs={\"hnsw_ef_search\": 256}\n",
    ")\n",
    "\n",
    "print(\">> Quick test on the RAG system.\")\n",
    "question = \"What are the main Hubble telescope discoveries about exoplanets?\"\n",
    "print(f\" > Question: {question}\")\n",
    "response = query_engine.query(question)\n",
    "print(f\" > Response:\\n\", response.response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:28:28.766821Z",
     "start_time": "2024-04-09T00:28:24.186275Z"
    }
   },
   "id": "5546df055ae0f2ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Quick test on the RAG system.\n",
      " > Question: What are the main Hubble telescope discoveries about exoplanets?\n",
      " > Response:\n",
      " \n",
      "\n",
      "The Hubble Space Telescope has revealed exceedingly valuable information about hundreds of other worlds, even though it was not designed with exoplanet science in mind. Hubble's observations have extended to Earth-size worlds and have even identified atmospheres that contain sodium, oxygen, carbon, hydrogen, carbon dioxide, methane, and water vapor. While most of the planets Hubble has studied to date are too hot to host life as we know it, the telescope's observations demonstrate that the basic organic components for life can be detected and measured on planets orbiting other stars, setting the stage for more detailed studies with future observatories. Hubble has also confrmed that a planet orbits two suns, and made a detailed global map of another world showing the temperature at different layers in its atmosphere and the amount and distribution of its water vapor. The mission's ongoing observations will continue to enhance our understanding of the cosmos and reveal new planetary wonders, propelling us forward in our search for life on other planets.\n",
      "CPU times: user 347 ms, sys: 124 ms, total: 470 ms\n",
      "Wall time: 4.57 s\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
