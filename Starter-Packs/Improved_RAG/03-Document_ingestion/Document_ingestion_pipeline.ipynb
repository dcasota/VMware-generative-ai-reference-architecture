{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Document Ingestion Pipeline\n",
    "\n",
    "__Please make sure you follow the instructions provided in the README file of this directory before starting the execution of this notebook.__"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82e3fed87148b4ef"
  },
  {
   "cell_type": "code",
   "id": "44f32ec9eeb84e66",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-26T00:50:54.283488Z",
     "start_time": "2024-08-26T00:50:45.932701Z"
    }
   },
   "source": [
    "import time\n",
    "# Start timer to time the notebook execution\n",
    "start = time.time()\n",
    "\n",
    "# General purpose imports\n",
    "import pandas as pd\n",
    "import ast\n",
    "import yaml\n",
    "from dotmap import DotMap\n",
    "import psycopg2\n",
    "from sqlalchemy import make_url\n",
    "from pprint import pprint\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "widgets.IntSlider()\n",
    "\n",
    "# LlamaIndex imports\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.embeddings.nvidia import NVIDIAEmbedding\n",
    "\n",
    "# Imports from this repo\n",
    "import sys\n",
    "utils_path = \"../08-Utils\"\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "\n",
    "from helpers import (\n",
    "    get_indices_with_nulls, \n",
    "    remove_elements,\n",
    "    TextCleaner\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Global Config Setup \n",
    "- Here we define most of the RAG pipeline parameters and LlamaIndex defaults.\n",
    "- The goal is to provide a single control cell to define the notebook's execution."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b61cdfb6594b23fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T00:50:54.311767Z",
     "start_time": "2024-08-26T00:50:54.285674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Open the Starter Pack global configuration file\n",
    "with open('../07-Starter_Pack_config/improved_rag_config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "config = DotMap(config)"
   ],
   "id": "8fa10d9d77f0b586",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T00:50:55.125106Z",
     "start_time": "2024-08-26T00:50:54.313307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# LLamaIndex LLM provider\n",
    "llm_cfg = config.ml_models.llm_generator\n",
    "Settings.llm = OpenAILike(\n",
    "    model=llm_cfg.model,\n",
    "    api_key=llm_cfg.api_key,\n",
    "    api_base=llm_cfg.api_base,\n",
    "    temperature=llm_cfg.temperature,\n",
    "    max_tokens=llm_cfg.max_tokens,\n",
    "    repetition_penalty=llm_cfg.repetition_penalty,\n",
    ")\n",
    "\n",
    "# LLamaIndex embedding model\n",
    "emb_cfg = config.ml_models.embedder\n",
    "Settings.embed_model = NVIDIAEmbedding(\n",
    "        base_url=emb_cfg.api_base,\n",
    "        model=emb_cfg.model,\n",
    "        embed_batch_size=emb_cfg.batch_size,\n",
    "        truncate=\"END\",\n",
    ")\n",
    "EMBEDDING_SIZE = len(Settings.embed_model.get_text_embedding(\"hi\"))"
   ],
   "id": "1f5645952b796250",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "92686bb0-85ed-4bb3-99eb-f5fc6c100787",
   "metadata": {
    "id": "92686bb0-85ed-4bb3-99eb-f5fc6c100787"
   },
   "source": "## PDF Documents Parsing"
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# Lamda function to add the file name as metadata at loading time\n",
    "filename_fn = lambda filename: {\"file_name\": filename.split(\"/\")[-1]}\n",
    "\n",
    "# >> PDF Document Parsing\n",
    "# The PyMuPDFReader takes ~ 1/20 the time it takes to the default reader to ingest the PDF files\n",
    "# Note: PyMuPDFReader creates a document object per page in a PDF document.\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir=config.file_paths.kb_doc_dir,\n",
    "    required_exts=[\".pdf\"],\n",
    "    file_extractor={\".pdf\":PyMuPDFReader()},\n",
    "    file_metadata=filename_fn,\n",
    "    num_files_limit=10,\n",
    ")\n",
    "documents = reader.load_data()\n",
    "\n",
    "# Filter out documents with null (`\\x00') characters which are incompatible with PGVector.\n",
    "bad_docs = get_indices_with_nulls(documents)\n",
    "documents = remove_elements(documents, bad_docs)\n",
    "print(f\"Parsing is complete. Loaded {len(documents)} pages\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-26T00:51:41.440473Z",
     "start_time": "2024-08-26T00:50:55.126887Z"
    }
   },
   "id": "9ee792fdb855fc12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing is complete. Loaded 3855 pages\n",
      "CPU times: user 33.1 s, sys: 12.6 s, total: 45.6 s\n",
      "Wall time: 46.3 s\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T00:51:41.447700Z",
     "start_time": "2024-08-26T00:51:41.443626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display one document as example\n",
    "example_doc = 10\n",
    "print(f\">> Document #{example_doc}'s text:\\n\", documents[example_doc], \"\\n\")\n",
    "print(f\">> Document #{example_doc}'s metadata:\")\n",
    "pprint(documents[example_doc].metadata, depth=1, indent=4, width=100)"
   ],
   "id": "f674caa1b8cec1e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Document #10's text:\n",
      " Doc ID: b55ed939-3e75-4dde-975f-284798f12b94\n",
      "Text: 1 INTRODUCTION NASA’s Solar System Exploration Paradigm:  The\n",
      "First 50 Years and a  Look at the Next 50 James L. Green and Kristen\n",
      "J. Erickson A FTER MANY FAILURES to get to the Moon and to the planets\n",
      "beyond,  Mariner 2 successfully flew by Venus in December 1962. This\n",
      "historic  mission began a spectacular era of solar system exploration\n",
      "for NA... \n",
      "\n",
      ">> Document #10's metadata:\n",
      "{   'file_name': '50-years-of-solar-system-exploration_tagged.pdf',\n",
      "    'file_path': '/Users/kike/DataspellProjects/Improved_RAG/Starter-Packs/Improved_RAG/03-Document_ingestion/../02-KB-Documents/NASA/50-years-of-solar-system-exploration_tagged.pdf',\n",
      "    'source': '11',\n",
      "    'total_pages': 364}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Documents metrics analysis\n",
    "This section is useful to get a better understanding about th text data and its possible effects on LLM inference timeouts. Notice that documents with few pages had too few or no words at all might make the vLLM inference service to fail."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a15efa854825b5fb"
  },
  {
   "cell_type": "code",
   "source": [
    "# Helper dict to form a dataframe\n",
    "docs_metrics = {\n",
    "    'words': [],\n",
    "    'text': [],\n",
    "}\n",
    "for doc in documents:\n",
    "    docs_metrics['words'].append(len(doc.text.split()))\n",
    "    docs_metrics['text'].append(doc.text)\n",
    "docs_metrics = pd.DataFrame.from_dict(docs_metrics)\n",
    "\n",
    "# Get statistics about number of words in documents\n",
    "## Notice the presence of docs with 0 words\n",
    "print(\"Words per page statistics\")\n",
    "display(docs_metrics.words.describe())\n",
    "\n",
    "# Remove \"too small\" pages from the corpus\n",
    "lindex_cfg = config.llama_index\n",
    "short_docs = docs_metrics.words[docs_metrics.words<lindex_cfg.min_doc_length].index.to_list()\n",
    "print(f\">> Removing {len(short_docs)} pages from the corpus\")\n",
    "documents = remove_elements(documents, short_docs)\n",
    "print(f\" > New pages list size: {len(documents)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-26T00:51:41.580186Z",
     "start_time": "2024-08-26T00:51:41.449559Z"
    }
   },
   "id": "40000930e2ef4b1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words per page statistics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    3855.000000\n",
       "mean      383.659403\n",
       "std       156.257311\n",
       "min         0.000000\n",
       "25%       304.000000\n",
       "50%       412.000000\n",
       "75%       474.000000\n",
       "max      2775.000000\n",
       "Name: words, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Removing 163 pages from the corpus\n",
      " > New pages list size: 3692\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "5b00be91-22ea-403c-b9c4-cd030b7e6c09",
   "metadata": {
    "id": "5b00be91-22ea-403c-b9c4-cd030b7e6c09"
   },
   "source": "## Setup and run the ingestion pipeline"
  },
  {
   "cell_type": "code",
   "id": "1089adee-bc8a-457f-8d96-113435923d10",
   "metadata": {
    "id": "1089adee-bc8a-457f-8d96-113435923d10",
    "ExecuteTime": {
     "end_time": "2024-08-26T00:52:52.665674Z",
     "start_time": "2024-08-26T00:51:41.581772Z"
    }
   },
   "source": [
    "%%time\n",
    "\n",
    "# Create the pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        # Remove troublesome characters\n",
    "        TextCleaner(),\n",
    "        # Split docs with preference to complete sentences\n",
    "        SentenceSplitter(\n",
    "        # TokenTextSplitter(\n",
    "                chunk_size=lindex_cfg.chunk_size,\n",
    "                chunk_overlap=lindex_cfg.chunk_overlap,\n",
    "                include_metadata=False,\n",
    "        ),\n",
    "        # Generate embeddings for document splits\n",
    "        Settings.embed_model,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Run the ingestion pipeline\n",
    "nodes = pipeline.run(\n",
    "    show_progress=True,\n",
    "    documents=documents,\n",
    "    num_workers=lindex_cfg.num_workers,\n",
    ")\n",
    "print(f\">> Created {len(nodes)} nodes.\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parsing nodes:   0%|          | 0/3692 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2545d48272b8427e9699258ed48fd9ad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating embeddings:   0%|          | 0/7053 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11b94c974bf9496ba1e9bc98bad76154"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Created 7053 nodes.\n",
      "CPU times: user 13.8 s, sys: 1.16 s, total: 15 s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Displaying a node's structure: text + metadata",
   "id": "8da5499dfb182fe6"
  },
  {
   "cell_type": "code",
   "source": [
    "# Show the aspect of LlamaIndex nodes and their metadata.\n",
    "print(\">> LlamaIndex node's metadata sample:\")\n",
    "print(nodes[0].to_dict()['metadata'], \"\\n\")\n",
    "print(\">> LlamaIndex node's text excerpt:\\n\", \n",
    "      nodes[0].text,\n",
    "      \"...\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-26T00:52:52.670388Z",
     "start_time": "2024-08-26T00:52:52.667145Z"
    }
   },
   "id": "4ef40a0334af2540",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> LlamaIndex node's metadata sample:\n",
      "{} \n",
      "\n",
      ">> LlamaIndex node's text excerpt:\n",
      " HISTORICAL PERSPECTIVES NASA’S FIRST SUCCESSFUL MISSION to another planet,  Mariner 2 to Venus in 1962, marked the beginning of what NASA Chief Scientist Jim Green  describes in this volume as “a spectacular era”  of solar system exploration. In its first 50 years  of planetary exploration, NASA sent spacecraft  to fly by, orbit, land on, or rove on every planet  in our solar system, as well as Earth’s Moon and  several moons of other planets. Pluto, reclassified as a dwarf planet in 2006, was visited by  the New Horizons spacecraft in 2015. What began as an endeavor of two  nations—the United States and the former  Soviet Union—has become a multinational  enterprise, with a growing number of space  agencies worldwide building and launching planetary exploration missions—sometimes alone,  sometimes together.  In this volume, a diverse array of scholars addresses the science, technology, policy,  and politics of planetary exploration. This volume offers a collection of in-depth studies of  important projects, decisions, and milestones  of this era. It is not possible to foresee what the next  50 years of NASA’s planetary exploration program will reveal. However, the 2020s are already  looking promising. Planetary missions in recent  years have focused more and more on exploring potentially habitable environments in our  solar system and developing a more in-depth  understanding of the evolution of planetary  environments. Upcoming missions will continue  to do so. In 2020, NASA launched its Mars 2020  rover, Perseverance, and in 2022 the European  Space Agency will launch its Exomars rover.  The Lucy and Psyche asteroid missions are set  to launch in this decade, as are the Dragonfly    50 YEARS OF   SOLAR SYSTEM  EXPLORATION FRONT COVER: Solar system exploration illustration. Credit: NASA/ Jenny Mottar BACK COVER: Mariner 2 spacecraft. ...\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": "## Create a PGVector store in an existing PostgresSQL DB",
   "metadata": {
    "collapsed": false
   },
   "id": "516e7498a0ad598f"
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "# Connect to the PostgreSQL engine and initialize de DB to serve as vector/document store.\n",
    "db_cfg = config.postgresql\n",
    "connection_string = (f\"postgresql://{db_cfg.user}:\"\n",
    "                     f\"{db_cfg.password}@{db_cfg.db_host}:{db_cfg.port}/{db_cfg.default_db}\")\n",
    "conn = psycopg2.connect(connection_string)\n",
    "conn.autocommit = True\n",
    "\n",
    "# Create a url object to store DB connection parameters\n",
    "url = make_url(connection_string)\n",
    "conn = psycopg2.connect(connection_string)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(f\"DROP TABLE IF EXISTS public.data_{db_cfg.tables.std_rag};\")\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# Connect to the PGVector extension\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=url.database,\n",
    "    host=url.host,\n",
    "    password=url.password,\n",
    "    port=url.port,\n",
    "    user=url.username,\n",
    "    table_name=db_cfg.tables.std_rag,\n",
    "    embed_dim=EMBEDDING_SIZE, # embedding model dimension\n",
    "    cache_ok=True,\n",
    "    hybrid_search=db_cfg.pgvector.hybrid_search, # retrieve nodes based on vector values and keywords\n",
    ")\n",
    "\n",
    "# LlamaIndex persistence object backed by the PGVector connection\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# LlamaIndex index population (nodes -> embeddings -> vector store)\n",
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    storage_context=storage_context,\n",
    "    show_progress=True,\n",
    "    transformations=None,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-26T00:54:13.972967Z",
     "start_time": "2024-08-26T00:52:52.671869Z"
    }
   },
   "id": "90549c05a99c9ce6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating embeddings: 0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0792f576e1ac42c1bb99b0b712d90058"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating embeddings: 0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bac4f13273724aef995c127dbaa1cdce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating embeddings: 0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45e958b20ecf48faa9fde96ce0aec0ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating embeddings: 0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e0d02d2bee6486ea0d211a9fbb73059"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.02 s, sys: 821 ms, total: 7.84 s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RAG Pipeline Test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e061a41d60a4fd16"
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# Use the index as a query engine to give the LLM the required context\n",
    "## to answer questions about the NASA history knowledge domain.\n",
    "h_search = ast.literal_eval(db_cfg.pgvector.pgvector_kwargs)\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=Settings.llm,\n",
    "    similarity_top_k=5,\n",
    "    vector_store_kwargs=h_search\n",
    ")\n",
    "\n",
    "print(\">> Quick test on the RAG system.\\n\")\n",
    "question = (\"As a research assistant, \"\n",
    "            \"list the top 10 Hubble telescope discoveries about exoplanets. \"\n",
    "            \"Highlight important text using markdown formatting.\")\n",
    "print(f\" > Query: {question}\")\n",
    "response = query_engine.query(question)\n",
    "display(Markdown(response.response))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-26T00:54:18.707560Z",
     "start_time": "2024-08-26T00:54:13.975146Z"
    }
   },
   "id": "5546df055ae0f2ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Quick test on the RAG system.\n",
      "\n",
      " > Query: As a research assistant, list the top 10 Hubble telescope discoveries about exoplanets. Highlight important text using markdown formatting.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": " **Top 10 Hubble Telescope Discoveries about Exoplanets**\n\n1. **Probing the Atmospheres of Rocky, Habitable-Zone Planets**: Hubble studies exoplanet atmospheres spectroscopically during transits, helping astronomers understand what the atmospheres are made of. This offers clues to the planets' formation and evolution and hints at whether they are likely to be habitable.\n2. **Spotting a World with a Glowing Water Atmosphere**: Hubble detected water vapor in the atmosphere of a distant exoplanet, which could be a sign of life.\n3. **Discovering an Alien Atmosphere that is Brimming with Water**: Hubble found that a distant exoplanet has an atmosphere rich in water vapor, which could be a sign of life.\n4. **Detecting Water Vapor on a Habitable-Zone Exoplanet**: Hubble detected water vapor on a planet that orbits its star at a distance where liquid water could exist on its surface, making it a potential candidate for hosting life.\n5. **Exposing the First Evidence of a Possible Exomoon**: Hubble detected a possible moon orbiting a distant exoplanet, which could provide insights into the formation and evolution of planetary systems.\n6. **Capturing a Blistering Pitch-Black Planet**: Hubble captured an image of a planet with a surface that is as dark as coal, which could be due to the presence of organic material or other substances.\n7. **Finding a Shrinking Planet**: Hubble detected a planet that is shrinking due to the loss of mass, which could be a sign of a planet that is experiencing a catastrophic event.\n8. **Uncovering a Football-Shaped ‘Heavy Metal’ Exoplanet**: Hubble discovered a planet with a unique shape and composition, which could provide insights into the formation and evolution of planetary systems.\n9. **Unraveling Mysteries Surrounding ‘Cotton Candy’ Planets**: Hubble studied the atmospheres of exoplanets with unusual compositions, which could provide insights into the formation and evolution of planetary systems.\n10. **Tracking an Exiled Exoplanet’s Far-Flung Orbit**: Hubble tracked the orbit of a distant exoplanet that is no longer in its original orbit, which could provide insights into the formation and evolution of planetary systems.\n\nThese discoveries highlight the importance of the Hubble Space Telescope in advancing our understanding of exoplanets and their potential for hosting life. **[1](https://www.nasa.gov/hubble)** **[2](https://hubblesite.org)** **[3](https://www.facebook.com/NASAHubble)** **[4](https://twitter.com/NASAHubble)** **[5](https://www.instagram.com/NASAHubble)** **[6](https://www.youtube.com/playlist?list=PL3E861DC9F9A8F2E9)** **[7](https://www.flickr.com/photos/nasahubble)** **[8](https://www.pinterest.com/nasa/hubble-space-telescope/)**\n\nNote: The text in bold is the original text from the provided context information. The rest of the answer is a summary and explanation of the top 10 Hubble telescope discoveries about exoplanets."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 60.5 ms, sys: 6.48 ms, total: 66.9 ms\n",
      "Wall time: 4.73 s\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T00:54:18.712059Z",
     "start_time": "2024-08-26T00:54:18.709190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stop = time.time()\n",
    "print(f\"Notebook execution time: {(stop-start)/60:.1f} minutes\")"
   ],
   "id": "32cf917944f61a66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook execution time: 3.5 minutes\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
