# Improved RAG Starter Pack

The **Improved RAG Starter Pack** offers a comprehensive suite of Python scripts designed for enthusiasts and 
professionals in machine learning, specifically focusing on Retrieval-Augmented Generation (RAG) pipelines.
This repository provides step-by-step guidance on how to build a RAG system, starting from the document ingestion,
vector (text embeddings) index population, and standard RAG pipeline implementation, all the way to improvements over
the standard RAG retrieval processes. We do this by leveraging the power of __[LlamaIndex v0.10.27](https://www.llamaindex.ai/open-source) and 
[PGVector](https://github.com/pgvector/pgvector)__ for enhanced performance in question-answering systems. 

### Here's what it covers:

- **E-Books Ingestion into PGVector DB**: Demonstrates how to ingest a collection of e-books on NASA's history into a 
PGVector database. This process enriches Large Language Models (LLMs) by providing a grounded foundation for 
answering questions related to NASA's institutional history and achievements in space exploration.

- **Standard RAG Pipeline + re-ranker**: Implements a standard RAG pipeline using LlamaIndex, incorporating a
final re-ranking step. The re-ranker enhances the quality of document retrieval and, subsequently, the answers 
generated by LLMs by selecting the most relevant document chunks to augment the generation process.

- **Sentence Windows Parsing**: Improves the standard RAG pipeline by introducing LlamaIndex's implementation of 
Sentence Windows Parsing. This method optimizes the parsing of retrieved documents, ensuring a more effective and 
contextually relevant selection of text for LLMs.

- **Auto Merging Retrieval**: Further refines the RAG pipeline through LlamaIndex's Auto Merging Retrieval feature. 
This technique automatically merges relevant retrieved information, streamlining the input to LLMs and fostering 
more coherent and accurate responses.

- **Evaluation Dataset Generation**: Provides instructions for generating an evaluation dataset tailored for assessing
RAG pipelines. This component is crucial for developers looking to benchmark and validate their implementations 
against standardized metrics such as __Contextual Precision, Faithfulness, Contextual Recall, and 
Answer Relevancy.__

- **RAG Pipeline Evaluation**: Outlines a methodology for evaluating RAG pipelines using the DeepEval library and 
a pre-generated evaluation dataset. This approach allows for a detailed assessment of a pipeline's performance, 
offering insights into its efficiency based on __Contextual Precision, Faithfulness, Contextual Recall, and
  Answer Relevancy__ metrics.

This repository is a practical resource for developers and researchers aiming to advance their understanding 
and application of retrieval-augmented generation techniques. Whether you're looking to enhance an existing project 
or embark on a new one, the **Improved RAG Starter Pack** provides the tools and knowledge needed to __build, evaluate,
and improve__ your RAG systems and applications.

### Starter Pack Directory Structure
To run all the examples in the Starter Pack, please execute the tasks following the numeric order of the directory
structure. __Each directory contains a README.md file with instructions on how to use its content.__
```
├── 01-PGVector (START HERE)  
├── 02-KB-Documents  
│   └── NASA  
├── 03-Document_ingestion  
├── 04-RAG_Variants  
│   ├── 01-Simple_Retrieval  
│   ├── 02-Sentence_Window_Retrieval  
│   └── 03-Auto_Merging_Retrieval  
├── 05-RAG_Dataset_Generation  
│   └── LlamaIndex_generation  
│       └── qa_datasets  
└── 06-RAG_System_Evaluation  
```
