# Improved RAG Starter Pack

The **Improved RAG Starter Pack** offers a comprehensive suite of Python scripts designed for enthusiasts and 
researchers in the domain of machine learning, specifically focusing on Retrieval-Augmented Generation (RAG) pipelines.
This repository is a treasure trove for those looking to delve into advanced natural language processing techniques, 
leveraging the power of __[LlamaIndex](https://www.llamaindex.ai/open-source) and 
[PGVector](https://github.com/pgvector/pgvector)__ for enhanced performance in question-answering systems. 

### Here's what it covers:

- **E-Books Ingestion into PGVector DB**: Demonstrates how to ingest a collection of e-books on NASA's history into a 
PGVector database. This process enriches Language Learning Models (LLMs) by providing a robust foundation for 
answering questions related to NASA's institutional history and achievements in space exploration.

- **Standard RAG Pipeline + re-ranker**: Implements a standard RAG pipeline using LlamaIndex, incorporating a
final re-ranking step. The re-ranker enhances the quality of document retrieval and, subsequently, the answers 
generated by LLMs by selecting the most relevant document chunks to augment the generation process.

- **Sentence Windows Parsing**: Improves the standard RAG pipeline by introducing LlamaIndex's implementation of 
Sentence Windows Parsing. This method optimizes the parsing of retrieved documents, ensuring a more effective and 
contextually relevant selection of text for LLMs.

- **Auto Merging Retrieval**: Further refines the RAG pipeline through LlamaIndex's Auto Merging Retrieval feature. 
This technique automatically merges relevant retrieved information, streamlining the input to LLMs and fostering 
more coherent and accurate responses.

- **Evaluation Dataset Generation**: Provides instructions for generating an evaluation dataset tailored for assessing
RAG pipelines. This component is crucial for developers looking to benchmark and validate their implementations 
against standardized metrics such as:```Contextual Precision, Faithfulness, Contextual Recall and 
Answer Relevancy.```

- **RAG Pipeline Evaluation**: Outlines a methodology for evaluating RAG pipelines using the DeepEval library and 
a pre-generated evaluation dataset. This approach allows for a detailed assessment of a pipeline's performance, 
offering insights into its efficiency based on the __Contextual Precision, Faithfulness, Contextual Recall and
  Answer Relevancy__ metrics.

This repository serves as a practical resource for developers and researchers aiming to advance their understanding 
and application of retrieval-augmented generation techniques. Whether you're looking to enhance an existing project 
or embark on a new one, the **Improved RAG Starter Pack** provides the tools and knowledge needed to __build, evaluate,
and improve__ your RAG systems and applications.

### Starter Pack Directory Structure
To run all the examples in the Starter Pack, please execute the tasks following the numeric order of the directory
structure. __Each directory contains a README.md file with the instructions on how to use its content.__

├── __01-PGVector (START HERE)__  
├── __02-KB-Documents__  
│   └── NASA  
├── __03-Document_ingestion__  
├── __04-RAG_Variants__  
│   ├── 01-Simple_Retrieval  
│   ├── 02-Sentence_Window_Retrieval  
│   └── 03-Auto_Merging_Retrieval  
├── __05-RAG_Dataset_Generation__  
│   ├── LlamaIndex_generation  
│       └── qa_datasets  
├── __06-RAG_System_Evaluation__  
